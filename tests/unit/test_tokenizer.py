# -*- coding: utf-8 -*-

"""
Unit-—Ç–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (kiro/tokenizer.py).

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
- –ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ (count_tokens)
- –ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö (count_message_tokens)
- –ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ö (count_tools_tokens)
- –û—Ü–µ–Ω–∫—É —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ (estimate_request_tokens)
- –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –¥–ª—è Claude (CLAUDE_CORRECTION_FACTOR)
- Fallback –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ tiktoken
"""

import pytest
from unittest.mock import patch, MagicMock

from kiro.tokenizer import (
    count_tokens,
    count_message_tokens,
    count_tools_tokens,
    estimate_request_tokens,
    count_anthropic_content_block_tokens,
    count_anthropic_message_tokens,
    count_anthropic_tools_tokens,
    count_anthropic_system_tokens,
    estimate_anthropic_request_tokens,
    CLAUDE_CORRECTION_FACTOR,
    _get_encoding
)


class TestCountTokens:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ count_tokens."""
    
    def test_empty_string_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥—Ä–∞–Ω–∏—á–Ω–æ–≥–æ —Å–ª—É—á–∞—è.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞...")
        result = count_tokens("")
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "–ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_none_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ None –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ None.
        """
        print("–¢–µ—Å—Ç: None...")
        result = count_tokens(None)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "None –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_simple_text_returns_positive(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø–æ–¥—Å—á—ë—Ç–∞.
        """
        print("–¢–µ—Å—Ç: –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç...")
        result = count_tokens("Hello, world!")
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result > 0, "–ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_longer_text_returns_more_tokens(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥—Å—á—ë—Ç–∞.
        """
        print("–¢–µ—Å—Ç: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∏ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞...")
        short_text = "Hello"
        long_text = "Hello, this is a much longer text that should have more tokens"
        
        short_tokens = count_tokens(short_text)
        long_tokens = count_tokens(long_text)
        
        print(f"–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç: {short_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        print(f"–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {long_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        
        assert long_tokens > short_tokens, "–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_claude_correction_applied_by_default(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ Claude –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ apply_claude_correction=True –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
        """
        print("–¢–µ—Å—Ç: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ Claude...")
        text = "This is a test text for token counting"
        
        with_correction = count_tokens(text, apply_claude_correction=True)
        without_correction = count_tokens(text, apply_claude_correction=False)
        
        print(f"–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π: {with_correction}")
        print(f"–ë–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {without_correction}")
        
        # –° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç 1.15)
        assert with_correction > without_correction, "–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–º–µ—Ä–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ
        ratio = with_correction / without_correction
        print(f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {ratio}")
        assert 1.1 <= ratio <= 1.2, f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–∫–æ–ª–æ {CLAUDE_CORRECTION_FACTOR}"
    
    def test_without_claude_correction(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –±–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ apply_claude_correction=False —Ä–∞–±–æ—Ç–∞–µ—Ç.
        """
        print("–¢–µ—Å—Ç: –ë–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏...")
        text = "Test text"
        
        result = count_tokens(text, apply_claude_correction=False)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_unicode_text(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è Unicode —Ç–µ–∫—Å—Ç–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–µ-ASCII —Å–∏–º–≤–æ–ª–æ–≤.
        """
        print("–¢–µ—Å—Ç: Unicode —Ç–µ–∫—Å—Ç...")
        text = "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! ‰Ω†Â•Ω‰∏ñÁïå üåç"
        
        result = count_tokens(text)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "Unicode —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_multiline_text(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫.
        """
        print("–¢–µ—Å—Ç: –ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç...")
        text = """Line 1
        Line 2
        Line 3"""
        
        result = count_tokens(text)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_json_text(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è JSON —Å—Ç—Ä–æ–∫–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ JSON.
        """
        print("–¢–µ—Å—Ç: JSON —Ç–µ–∫—Å—Ç...")
        text = '{"name": "test", "value": 123, "nested": {"key": "value"}}'
        
        result = count_tokens(text)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "JSON —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"


class TestCountTokensFallback:
    """–¢–µ—Å—Ç—ã –¥–ª—è fallback –ª–æ–≥–∏–∫–∏ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ tiktoken."""
    
    def test_fallback_when_tiktoken_unavailable(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç fallback –ø–æ–¥—Å—á—ë—Ç –∫–æ–≥–¥–∞ tiktoken –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ tiktoken.
        """
        print("–¢–µ—Å—Ç: Fallback –±–µ–∑ tiktoken...")
        
        # –ú–æ–∫–∏—Ä—É–µ–º _get_encoding —á—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å None
        with patch('kiro.tokenizer._get_encoding', return_value=None):
            result = count_tokens("Hello world test")
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
            
            # Fallback: len(text) // 4 + 1, –∑–∞—Ç–µ–º * 1.15
            # "Hello world test" = 16 —Å–∏–º–≤–æ–ª–æ–≤
            # 16 // 4 + 1 = 5
            # 5 * 1.15 = 5.75 -> 5
            assert result > 0, "Fallback –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ"
    
    def test_fallback_without_correction(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç fallback –±–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ fallback —Ä–∞–±–æ—Ç–∞–µ—Ç —Å apply_claude_correction=False.
        """
        print("–¢–µ—Å—Ç: Fallback –±–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏...")
        
        with patch('kiro.tokenizer._get_encoding', return_value=None):
            result = count_tokens("Test", apply_claude_correction=False)
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
            
            # "Test" = 4 —Å–∏–º–≤–æ–ª–∞
            # 4 // 4 + 1 = 2
            assert result > 0, "Fallback –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ"


class TestCountMessageTokens:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ count_message_tokens."""
    
    def test_empty_list_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π...")
        result = count_message_tokens([])
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "–ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_none_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ None –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ None.
        """
        print("–¢–µ—Å—Ç: None...")
        result = count_message_tokens(None)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "None –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_single_user_message(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ user —Å–æ–æ–±—â–µ–Ω–∏—è.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.
        """
        print("–¢–µ—Å—Ç: –û–¥–Ω–æ user —Å–æ–æ–±—â–µ–Ω–∏–µ...")
        messages = [{"role": "user", "content": "Hello, AI!"}]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_multiple_messages(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Ç–æ–∫–µ–Ω—ã —Å—É–º–º–∏—Ä—É—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.
        """
        print("–¢–µ—Å—Ç: –ù–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π...")
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"},
            {"role": "assistant", "content": "Hi there! How can I help you?"},
            {"role": "user", "content": "What is the weather?"}
        ]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        # –ë–æ–ª—å—à–µ —Å–æ–æ–±—â–µ–Ω–∏–π = –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤
        single_message = count_message_tokens([messages[0]])
        assert result > single_message, "–ù–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_message_with_tool_calls(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è —Å tool_calls.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ tool_calls —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –°–æ–æ–±—â–µ–Ω–∏–µ —Å tool_calls...")
        messages = [
            {
                "role": "assistant",
                "content": "",
                "tool_calls": [
                    {
                        "id": "call_123",
                        "type": "function",
                        "function": {
                            "name": "get_weather",
                            "arguments": '{"location": "Moscow"}'
                        }
                    }
                ]
            }
        ]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–°–æ–æ–±—â–µ–Ω–∏–µ —Å tool_calls –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_message_with_tool_call_id(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è tool response —Å–æ–æ–±—â–µ–Ω–∏—è.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ tool_call_id —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: Tool response —Å–æ–æ–±—â–µ–Ω–∏–µ...")
        messages = [
            {
                "role": "tool",
                "content": "The weather in Moscow is sunny, 25¬∞C",
                "tool_call_id": "call_123"
            }
        ]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "Tool response –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_message_with_list_content(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ list content –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç...")
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "What is in this image?"},
                    {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
                ]
            }
        ]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_without_claude_correction(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –±–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ apply_claude_correction=False —Ä–∞–±–æ—Ç–∞–µ—Ç.
        """
        print("–¢–µ—Å—Ç: –ë–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏...")
        messages = [{"role": "user", "content": "Test message"}]
        
        with_correction = count_message_tokens(messages, apply_claude_correction=True)
        without_correction = count_message_tokens(messages, apply_claude_correction=False)
        
        print(f"–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π: {with_correction}")
        print(f"–ë–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {without_correction}")
        
        assert with_correction > without_correction, "–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ"
    
    def test_message_with_empty_content(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è —Å –ø—É—Å—Ç—ã–º content.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –ø—É—Å—Ç–æ–π content –Ω–µ –ª–æ–º–∞–µ—Ç –ø–æ–¥—Å—á—ë—Ç.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç–æ–π content...")
        messages = [{"role": "user", "content": ""}]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (role, —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏)
        assert result > 0, "–î–∞–∂–µ –ø—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã"
    
    def test_message_with_none_content(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è —Å None content.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ None content –Ω–µ –ª–æ–º–∞–µ—Ç –ø–æ–¥—Å—á—ë—Ç.
        """
        print("–¢–µ—Å—Ç: None content...")
        messages = [{"role": "assistant", "content": None}]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–°–æ–æ–±—â–µ–Ω–∏–µ —Å None content –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã"


class TestCountToolsTokens:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ count_tools_tokens."""
    
    def test_none_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ None –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ None.
        """
        print("–¢–µ—Å—Ç: None...")
        result = count_tools_tokens(None)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "None –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_empty_list_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫...")
        result = count_tools_tokens([])
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "–ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_single_tool(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.
        """
        print("–¢–µ—Å—Ç: –û–¥–∏–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get the current weather for a location",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {"type": "string", "description": "City name"}
                        },
                        "required": ["location"]
                    }
                }
            }
        ]
        
        result = count_tools_tokens(tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_multiple_tools(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Ç–æ–∫–µ–Ω—ã —Å—É–º–º–∏—Ä—É—é—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –ù–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get weather",
                    "parameters": {"type": "object", "properties": {}}
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "search_web",
                    "description": "Search the web",
                    "parameters": {"type": "object", "properties": {}}
                }
            }
        ]
        
        result = count_tools_tokens(tools)
        single_tool = count_tools_tokens([tools[0]])
        
        print(f"–î–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {result}")
        print(f"–û–¥–∏–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {single_tool}")
        
        assert result > single_tool, "–ë–æ–ª—å—à–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ = –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_tool_with_complex_parameters(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ JSON schema –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –°–ª–æ–∂–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "complex_function",
                    "description": "A function with complex parameters",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string", "description": "Name"},
                            "age": {"type": "integer", "description": "Age"},
                            "address": {
                                "type": "object",
                                "properties": {
                                    "street": {"type": "string"},
                                    "city": {"type": "string"},
                                    "country": {"type": "string"}
                                }
                            },
                            "tags": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        },
                        "required": ["name", "age"]
                    }
                }
            }
        ]
        
        result = count_tools_tokens(tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–°–ª–æ–∂–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_tool_without_parameters(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ parameters –Ω–µ –ª–æ–º–∞–µ—Ç –ø–æ–¥—Å—á—ë—Ç.
        """
        print("–¢–µ—Å—Ç: –ë–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "no_params_func",
                    "description": "A function without parameters"
                }
            }
        ]
        
        result = count_tools_tokens(tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_tool_with_empty_description(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Å –ø—É—Å—Ç—ã–º description.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –ø—É—Å—Ç–æ–π description –Ω–µ –ª–æ–º–∞–µ—Ç –ø–æ–¥—Å—á—ë—Ç.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç–æ–π description...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "func",
                    "description": "",
                    "parameters": {"type": "object", "properties": {}}
                }
            }
        ]
        
        result = count_tools_tokens(tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Å –ø—É—Å—Ç—ã–º description –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_non_function_tool_type(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Å type != "function".
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ non-function tools –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: Non-function tool...")
        tools = [
            {
                "type": "other_type",
                "some_field": "value"
            }
        ]
        
        result = count_tools_tokens(tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ö–æ—Ç—è –±—ã —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        assert result >= 0, "Non-function tool –Ω–µ –¥–æ–ª–∂–µ–Ω –ª–æ–º–∞—Ç—å –ø–æ–¥—Å—á—ë—Ç"
    
    def test_without_claude_correction(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –±–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ apply_claude_correction=False —Ä–∞–±–æ—Ç–∞–µ—Ç.
        """
        print("–¢–µ—Å—Ç: –ë–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "test_func",
                    "description": "Test function",
                    "parameters": {"type": "object", "properties": {}}
                }
            }
        ]
        
        with_correction = count_tools_tokens(tools, apply_claude_correction=True)
        without_correction = count_tools_tokens(tools, apply_claude_correction=False)
        
        print(f"–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π: {with_correction}")
        print(f"–ë–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {without_correction}")
        
        assert with_correction > without_correction, "–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ"


class TestEstimateRequestTokens:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ estimate_request_tokens."""
    
    def test_messages_only(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ç–æ–∫–µ–Ω–æ–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.
        """
        print("–¢–µ—Å—Ç: –¢–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏—è...")
        messages = [{"role": "user", "content": "Hello!"}]
        
        result = estimate_request_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert "messages_tokens" in result
        assert "tools_tokens" in result
        assert "system_tokens" in result
        assert "total_tokens" in result
        
        assert result["messages_tokens"] > 0
        assert result["tools_tokens"] == 0
        assert result["system_tokens"] == 0
        assert result["total_tokens"] == result["messages_tokens"]
    
    def test_messages_with_tools(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ tools —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –°–æ–æ–±—â–µ–Ω–∏—è —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏...")
        messages = [{"role": "user", "content": "What is the weather?"}]
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get weather",
                    "parameters": {"type": "object", "properties": {}}
                }
            }
        ]
        
        result = estimate_request_tokens(messages, tools=tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result["messages_tokens"] > 0
        assert result["tools_tokens"] > 0
        assert result["total_tokens"] == result["messages_tokens"] + result["tools_tokens"]
    
    def test_messages_with_system_prompt(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ç–æ–∫–µ–Ω–æ–≤ —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º system prompt.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ system_prompt —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –° system prompt...")
        messages = [{"role": "user", "content": "Hello!"}]
        system_prompt = "You are a helpful assistant."
        
        result = estimate_request_tokens(messages, system_prompt=system_prompt)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result["messages_tokens"] > 0
        assert result["system_tokens"] > 0
        assert result["total_tokens"] == result["messages_tokens"] + result["system_tokens"]
    
    def test_full_request(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å—É–º–º–∏—Ä—É—é—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –ü–æ–ª–Ω—ã–π –∑–∞–ø—Ä–æ—Å...")
        messages = [
            {"role": "user", "content": "What is the weather in Moscow?"}
        ]
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get weather for a location",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {"type": "string"}
                        }
                    }
                }
            }
        ]
        system_prompt = "You are a weather assistant."
        
        result = estimate_request_tokens(messages, tools=tools, system_prompt=system_prompt)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        expected_total = result["messages_tokens"] + result["tools_tokens"] + result["system_tokens"]
        assert result["total_tokens"] == expected_total, "Total –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—É–º–º–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"
    
    def test_empty_messages(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ü–µ–Ω–∫—É –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥—Ä–∞–Ω–∏—á–Ω–æ–≥–æ —Å–ª—É—á–∞—è.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è...")
        result = estimate_request_tokens([])
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result["messages_tokens"] == 0
        assert result["total_tokens"] == 0


class TestClaudeCorrectionFactor:
    """–¢–µ—Å—Ç—ã –¥–ª—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ Claude."""
    
    def test_correction_factor_value(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞–≤–µ–Ω 1.15.
        """
        print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {CLAUDE_CORRECTION_FACTOR}")
        assert CLAUDE_CORRECTION_FACTOR == 1.15, "–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 1.15"
    
    def test_correction_increases_token_count(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ü–∏—è —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.
        """
        print("–¢–µ—Å—Ç: –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã...")
        text = "This is a test text for checking the correction factor"
        
        with_correction = count_tokens(text, apply_claude_correction=True)
        without_correction = count_tokens(text, apply_claude_correction=False)
        
        print(f"–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π: {with_correction}")
        print(f"–ë–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {without_correction}")
        
        assert with_correction > without_correction
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–∞–∑–Ω–∏—Ü–∞ –ø—Ä–∏–º–µ—Ä–Ω–æ 15%
        increase_percent = (with_correction - without_correction) / without_correction * 100
        print(f"–£–≤–µ–ª–∏—á–µ–Ω–∏–µ: {increase_percent:.1f}%")
        
        # –î–æ–ø—É—Å–∫–∞–µ–º –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å –∏–∑-–∑–∞ –æ–∫—Ä—É–≥–ª–µ–Ω–∏—è
        assert 10 <= increase_percent <= 20, "–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–∫–æ–ª–æ 15%"
class TestGetEncoding:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ _get_encoding."""
    
    def test_returns_encoding_when_tiktoken_available(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ _get_encoding –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç encoding –∫–æ–≥–¥–∞ tiktoken –¥–æ—Å—Ç—É–ø–µ–Ω.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ tiktoken.
        """
        print("–¢–µ—Å—Ç: tiktoken –¥–æ—Å—Ç—É–ø–µ–Ω...")
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
        import kiro.tokenizer as tokenizer_module
        original_encoding = tokenizer_module._encoding
        tokenizer_module._encoding = None
        
        try:
            encoding = _get_encoding()
            print(f"Encoding: {encoding}")
            
            # –ï—Å–ª–∏ tiktoken —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å encoding
            if encoding is not None:
                assert hasattr(encoding, 'encode'), "Encoding –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –º–µ—Ç–æ–¥ encode"
        finally:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
            tokenizer_module._encoding = original_encoding
    
    def test_caches_encoding(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ encoding –∫—ç—à–∏—Ä—É–µ—Ç—Å—è.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –ª–µ–Ω–∏–≤–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.
        """
        print("–¢–µ—Å—Ç: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ encoding...")
        
        encoding1 = _get_encoding()
        encoding2 = _get_encoding()
        
        print(f"Encoding 1: {encoding1}")
        print(f"Encoding 2: {encoding2}")
        
        # –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å —Ç–æ—Ç –∂–µ –æ–±—ä–µ–∫—Ç
        assert encoding1 is encoding2, "Encoding –¥–æ–ª–∂–µ–Ω –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å—Å—è"
    
    def test_handles_import_error(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É ImportError –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ tiktoken.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ tiktoken.
        """
        print("–¢–µ—Å—Ç: ImportError...")
        
        import kiro.tokenizer as tokenizer_module
        original_encoding = tokenizer_module._encoding
        tokenizer_module._encoding = None
        
        try:
            # –ú–æ–∫–∏—Ä—É–µ–º import tiktoken —á—Ç–æ–±—ã –≤—ã–±—Ä–æ—Å–∏—Ç—å ImportError
            with patch.dict('sys.modules', {'tiktoken': None}):
                with patch('builtins.__import__', side_effect=ImportError("No module named 'tiktoken'")):
                    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∫—ç—à
                    tokenizer_module._encoding = None
                    
                    # –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å None –∏ –Ω–µ —É–ø–∞—Å—Ç—å
                    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –∏–∑-–∑–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —ç—Ç–æ—Ç —Ç–µ—Å—Ç –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –∏–¥–µ–∞–ª—å–Ω–æ
                    # –Ω–æ –≥–ª–∞–≤–Ω–æ–µ - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –∫–æ–¥ –Ω–µ –ø–∞–¥–∞–µ—Ç
                    pass
        finally:
            tokenizer_module._encoding = original_encoding


class TestTokenizerIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞."""
    
    def test_realistic_chat_request(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ chat –∑–∞–ø—Ä–æ—Å–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        """
        print("–¢–µ—Å—Ç: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π chat –∑–∞–ø—Ä–æ—Å...")
        
        messages = [
            {"role": "system", "content": "You are a helpful AI assistant. Be concise and accurate."},
            {"role": "user", "content": "What is the capital of France?"},
            {"role": "assistant", "content": "The capital of France is Paris."},
            {"role": "user", "content": "What is its population?"}
        ]
        
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "search_web",
                    "description": "Search the web for information",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "Search query"}
                        },
                        "required": ["query"]
                    }
                }
            }
        ]
        
        result = estimate_request_tokens(messages, tools=tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–π
        assert result["messages_tokens"] > 50, "–°–æ–æ–±—â–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å > 50 —Ç–æ–∫–µ–Ω–æ–≤"
        assert result["tools_tokens"] > 20, "Tools –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å > 20 —Ç–æ–∫–µ–Ω–æ–≤"
        assert result["total_tokens"] > 70, "Total –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 70 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_large_context(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
        """
        print("–¢–µ—Å—Ç: –ë–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç...")
        
        # –°–æ–∑–¥–∞—ë–º –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç
        large_text = "This is a test sentence. " * 1000  # ~5000 —Å–ª–æ–≤
        
        messages = [{"role": "user", "content": large_text}]
        
        result = estimate_request_tokens(messages)
        print(f"–¢–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª—å—à–æ–º —Ç–µ–∫—Å—Ç–µ: {result['total_tokens']}")
        
        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤
        assert result["total_tokens"] > 1000, "–ë–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å > 1000 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_consistency_across_calls(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ–¥—Å—á—ë—Ç–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω—ã.
        """
        print("–¢–µ—Å—Ç: –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å...")
        
        text = "This is a test for consistency checking"
        
        results = [count_tokens(text) for _ in range(5)]
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results}")
        
        # –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏
        assert len(set(results)) == 1, "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–º–∏"


# =============================================================================
# Tests for Anthropic-format token counting
# =============================================================================


class TestCountAnthropicContentBlockTokens:
    """Tests for count_anthropic_content_block_tokens function."""

    def test_text_block(self):
        """Counts tokens in a text content block."""
        block = {"type": "text", "text": "Hello, world!"}
        result = count_anthropic_content_block_tokens(block)
        assert result > 0

    def test_empty_text_block(self):
        """Empty text block returns 0 tokens."""
        block = {"type": "text", "text": ""}
        result = count_anthropic_content_block_tokens(block)
        assert result == 0

    def test_thinking_block(self):
        """Counts tokens in a thinking content block."""
        block = {"type": "thinking", "thinking": "Let me reason about this step by step..."}
        result = count_anthropic_content_block_tokens(block)
        assert result > 0

    def test_image_block_returns_fixed_estimate(self):
        """Image blocks return a fixed ~100 token estimate."""
        block = {"type": "image", "source": {"type": "base64", "media_type": "image/png", "data": "abc123"}}
        result = count_anthropic_content_block_tokens(block)
        assert result == 100

    def test_tool_use_block(self):
        """Counts tokens in a tool_use content block."""
        block = {
            "type": "tool_use",
            "id": "toolu_123",
            "name": "get_weather",
            "input": {"location": "Istanbul", "units": "celsius"}
        }
        result = count_anthropic_content_block_tokens(block)
        # Should include service tokens + name + input JSON
        assert result > 4

    def test_tool_use_block_empty_input(self):
        """Tool use with empty input still has service + name tokens."""
        block = {"type": "tool_use", "id": "toolu_123", "name": "ping", "input": {}}
        result = count_anthropic_content_block_tokens(block)
        assert result > 0

    def test_tool_result_block_string_content(self):
        """Counts tokens in a tool_result with string content."""
        block = {
            "type": "tool_result",
            "tool_use_id": "toolu_123",
            "content": "The weather in Istanbul is sunny, 28¬∞C"
        }
        result = count_anthropic_content_block_tokens(block)
        assert result > 4

    def test_tool_result_block_list_content(self):
        """Counts tokens in a tool_result with nested content blocks."""
        block = {
            "type": "tool_result",
            "tool_use_id": "toolu_123",
            "content": [
                {"type": "text", "text": "Result line 1"},
                {"type": "text", "text": "Result line 2"}
            ]
        }
        result = count_anthropic_content_block_tokens(block)
        assert result > 4

    def test_tool_result_block_empty_content(self):
        """Tool result with empty content still has service tokens."""
        block = {"type": "tool_result", "tool_use_id": "toolu_123", "content": ""}
        result = count_anthropic_content_block_tokens(block)
        # Service tokens + tool_use_id tokens
        assert result > 0

    def test_tool_result_with_nested_image(self):
        """Tool result containing an image in nested content."""
        block = {
            "type": "tool_result",
            "tool_use_id": "toolu_123",
            "content": [
                {"type": "text", "text": "Screenshot captured"},
                {"type": "image", "source": {"type": "base64", "media_type": "image/png", "data": "..."}}
            ]
        }
        result = count_anthropic_content_block_tokens(block)
        # Should include text tokens + 100 for image + service tokens
        assert result > 100

    def test_unknown_block_type_returns_zero(self):
        """Unknown block types return 0 tokens."""
        block = {"type": "unknown_type", "data": "something"}
        result = count_anthropic_content_block_tokens(block)
        assert result == 0

    def test_missing_type_returns_zero(self):
        """Block without type field returns 0 tokens."""
        block = {"text": "no type field"}
        result = count_anthropic_content_block_tokens(block)
        assert result == 0


class TestCountAnthropicMessageTokens:
    """Tests for count_anthropic_message_tokens function."""

    def test_empty_list_returns_zero(self):
        """Empty message list returns 0."""
        assert count_anthropic_message_tokens([]) == 0

    def test_none_returns_zero(self):
        """None returns 0."""
        assert count_anthropic_message_tokens(None) == 0

    def test_single_string_content_message(self):
        """Counts tokens for a message with string content."""
        messages = [{"role": "user", "content": "Hello, Claude!"}]
        result = count_anthropic_message_tokens(messages)
        assert result > 0

    def test_single_block_content_message(self):
        """Counts tokens for a message with content block list."""
        messages = [
            {
                "role": "user",
                "content": [{"type": "text", "text": "Hello, Claude!"}]
            }
        ]
        result = count_anthropic_message_tokens(messages)
        assert result > 0

    def test_multi_turn_conversation(self):
        """Multi-turn conversation has more tokens than single message."""
        single = [{"role": "user", "content": "Hi"}]
        multi = [
            {"role": "user", "content": "Hi"},
            {"role": "assistant", "content": "Hello! How can I help?"},
            {"role": "user", "content": "What is the weather?"},
        ]
        single_tokens = count_anthropic_message_tokens(single)
        multi_tokens = count_anthropic_message_tokens(multi)
        assert multi_tokens > single_tokens

    def test_message_with_tool_use_blocks(self):
        """Counts tokens for assistant message with tool_use blocks."""
        messages = [
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": "Let me check the weather."},
                    {
                        "type": "tool_use",
                        "id": "toolu_abc",
                        "name": "get_weather",
                        "input": {"location": "Paris"}
                    }
                ]
            }
        ]
        result = count_anthropic_message_tokens(messages)
        assert result > 0

    def test_thinking_blocks_skipped_in_assistant_messages(self):
        """Thinking blocks in assistant messages are NOT counted (Anthropic spec)."""
        messages_with_thinking = [
            {
                "role": "assistant",
                "content": [
                    {"type": "thinking", "thinking": "Let me reason step by step about this complex problem..."},
                    {"type": "text", "text": "The answer is 42."}
                ]
            }
        ]
        messages_without_thinking = [
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": "The answer is 42."}
                ]
            }
        ]
        with_thinking = count_anthropic_message_tokens(messages_with_thinking)
        without_thinking = count_anthropic_message_tokens(messages_without_thinking)
        assert with_thinking == without_thinking

    def test_thinking_blocks_skipped_only_for_assistant_role(self):
        """Thinking blocks are only skipped for assistant role, not user."""
        # User message with thinking block (unusual but should be counted)
        user_msg = [
            {
                "role": "user",
                "content": [
                    {"type": "thinking", "thinking": "Some thinking text here"},
                    {"type": "text", "text": "Hello"}
                ]
            }
        ]
        user_msg_no_thinking = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Hello"}
                ]
            }
        ]
        with_thinking = count_anthropic_message_tokens(user_msg)
        without_thinking = count_anthropic_message_tokens(user_msg_no_thinking)
        # User thinking blocks SHOULD be counted (not skipped)
        assert with_thinking > without_thinking

    def test_message_with_tool_result_blocks(self):
        """Counts tokens for user message with tool_result blocks."""
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "tool_result",
                        "tool_use_id": "toolu_abc",
                        "content": "Sunny, 25¬∞C in Paris"
                    }
                ]
            }
        ]
        result = count_anthropic_message_tokens(messages)
        assert result > 0

    def test_message_with_image_block(self):
        """Image blocks contribute ~100 tokens."""
        without_image = [{"role": "user", "content": [{"type": "text", "text": "Describe this"}]}]
        with_image = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Describe this"},
                    {"type": "image", "source": {"type": "base64", "media_type": "image/png", "data": "..."}}
                ]
            }
        ]
        tokens_without = count_anthropic_message_tokens(without_image)
        tokens_with = count_anthropic_message_tokens(with_image)
        assert tokens_with > tokens_without

    def test_claude_correction_applied(self):
        """Claude correction factor increases token count."""
        messages = [{"role": "user", "content": "Test message for correction factor"}]
        with_correction = count_anthropic_message_tokens(messages, apply_claude_correction=True)
        without_correction = count_anthropic_message_tokens(messages, apply_claude_correction=False)
        assert with_correction > without_correction

    def test_empty_content_message(self):
        """Message with empty string content still has service tokens."""
        messages = [{"role": "assistant", "content": ""}]
        result = count_anthropic_message_tokens(messages)
        assert result > 0

    def test_none_content_message(self):
        """Message with None content still has service tokens."""
        messages = [{"role": "assistant", "content": None}]
        result = count_anthropic_message_tokens(messages)
        assert result > 0


class TestCountAnthropicToolsTokens:
    """Tests for count_anthropic_tools_tokens function."""

    def test_none_returns_zero(self):
        """None returns 0."""
        assert count_anthropic_tools_tokens(None) == 0

    def test_empty_list_returns_zero(self):
        """Empty list returns 0."""
        assert count_anthropic_tools_tokens([]) == 0

    def test_single_tool(self):
        """Counts tokens for a single Anthropic tool definition."""
        tools = [
            {
                "name": "get_weather",
                "description": "Get the current weather for a location",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "location": {"type": "string", "description": "City name"}
                    },
                    "required": ["location"]
                }
            }
        ]
        result = count_anthropic_tools_tokens(tools)
        assert result > 0

    def test_multiple_tools(self):
        """More tools means more tokens."""
        one_tool = [
            {"name": "tool_a", "description": "Tool A", "input_schema": {"type": "object", "properties": {}}}
        ]
        two_tools = [
            {"name": "tool_a", "description": "Tool A", "input_schema": {"type": "object", "properties": {}}},
            {"name": "tool_b", "description": "Tool B", "input_schema": {"type": "object", "properties": {}}}
        ]
        assert count_anthropic_tools_tokens(two_tools) > count_anthropic_tools_tokens(one_tool)

    def test_tool_without_description(self):
        """Tool without description still has name + schema tokens."""
        tools = [
            {"name": "ping", "input_schema": {"type": "object", "properties": {}}}
        ]
        result = count_anthropic_tools_tokens(tools)
        assert result > 0

    def test_tool_with_complex_schema(self):
        """Complex input_schema produces more tokens."""
        simple = [
            {"name": "func", "description": "Simple", "input_schema": {"type": "object", "properties": {}}}
        ]
        complex_tool = [
            {
                "name": "func",
                "description": "Complex",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string", "description": "Full name"},
                        "age": {"type": "integer", "description": "Age in years"},
                        "address": {
                            "type": "object",
                            "properties": {
                                "street": {"type": "string"},
                                "city": {"type": "string"},
                                "zip": {"type": "string"}
                            }
                        }
                    },
                    "required": ["name", "age"]
                }
            }
        ]
        assert count_anthropic_tools_tokens(complex_tool) > count_anthropic_tools_tokens(simple)

    def test_claude_correction_applied(self):
        """Claude correction factor increases tool token count."""
        tools = [
            {"name": "test", "description": "A test tool", "input_schema": {"type": "object", "properties": {}}}
        ]
        with_correction = count_anthropic_tools_tokens(tools, apply_claude_correction=True)
        without_correction = count_anthropic_tools_tokens(tools, apply_claude_correction=False)
        assert with_correction > without_correction


class TestCountAnthropicSystemTokens:
    """Tests for count_anthropic_system_tokens function."""

    def test_none_returns_zero(self):
        """None system prompt returns 0."""
        assert count_anthropic_system_tokens(None) == 0

    def test_empty_string_returns_zero(self):
        """Empty string returns 0."""
        assert count_anthropic_system_tokens("") == 0

    def test_string_system_prompt(self):
        """Counts tokens for a plain string system prompt."""
        result = count_anthropic_system_tokens("You are a helpful assistant.")
        assert result > 0

    def test_list_system_prompt(self):
        """Counts tokens for a list-of-blocks system prompt (prompt caching format)."""
        system = [
            {"type": "text", "text": "You are a helpful assistant."},
            {"type": "text", "text": "Always be concise.", "cache_control": {"type": "ephemeral"}}
        ]
        result = count_anthropic_system_tokens(system)
        assert result > 0

    def test_list_system_prompt_matches_string(self):
        """List format with single block should be close to equivalent string."""
        text = "You are a helpful assistant."
        string_tokens = count_anthropic_system_tokens(text)
        list_tokens = count_anthropic_system_tokens([{"type": "text", "text": text}])
        # Should be the same since both contain the same text
        assert string_tokens == list_tokens

    def test_empty_list_returns_zero(self):
        """Empty list returns 0."""
        assert count_anthropic_system_tokens([]) == 0

    def test_claude_correction_applied(self):
        """Claude correction factor increases system token count."""
        system = "You are a helpful assistant that always provides detailed, accurate, and well-structured responses to user queries."
        with_correction = count_anthropic_system_tokens(system, apply_claude_correction=True)
        without_correction = count_anthropic_system_tokens(system, apply_claude_correction=False)
        assert with_correction > without_correction


class TestEstimateAnthropicRequestTokens:
    """Tests for estimate_anthropic_request_tokens function."""

    def test_messages_only(self):
        """Counts tokens for messages-only request."""
        messages = [{"role": "user", "content": "Hello!"}]
        result = estimate_anthropic_request_tokens(messages)
        assert result > 0

    def test_messages_with_tools(self):
        """Tools add to the total token count."""
        messages = [{"role": "user", "content": "Hello!"}]
        tools = [
            {"name": "get_weather", "description": "Get weather", "input_schema": {"type": "object", "properties": {}}}
        ]
        without_tools = estimate_anthropic_request_tokens(messages)
        with_tools = estimate_anthropic_request_tokens(messages, tools=tools)
        assert with_tools > without_tools

    def test_messages_with_system(self):
        """System prompt adds to the total token count."""
        messages = [{"role": "user", "content": "Hello!"}]
        without_system = estimate_anthropic_request_tokens(messages)
        with_system = estimate_anthropic_request_tokens(messages, system="You are a helpful assistant.")
        assert with_system > without_system

    def test_full_request(self):
        """Full request with messages + tools + system."""
        messages = [
            {"role": "user", "content": "What is the weather in Istanbul?"},
            {"role": "assistant", "content": [
                {"type": "tool_use", "id": "toolu_1", "name": "get_weather", "input": {"location": "Istanbul"}}
            ]},
            {"role": "user", "content": [
                {"type": "tool_result", "tool_use_id": "toolu_1", "content": "Sunny, 30¬∞C"}
            ]}
        ]
        tools = [
            {
                "name": "get_weather",
                "description": "Get weather for a location",
                "input_schema": {
                    "type": "object",
                    "properties": {"location": {"type": "string"}},
                    "required": ["location"]
                }
            }
        ]
        system = "You are a weather assistant."

        result = estimate_anthropic_request_tokens(messages, tools=tools, system=system)
        assert result > 50  # Should be a reasonable number for this request

    def test_empty_messages(self):
        """Empty messages returns 0 (or close to 0)."""
        result = estimate_anthropic_request_tokens([])
        assert result == 0

    def test_consistency(self):
        """Same input always produces same output."""
        messages = [{"role": "user", "content": "Deterministic test"}]
        results = [estimate_anthropic_request_tokens(messages) for _ in range(5)]
        assert len(set(results)) == 1